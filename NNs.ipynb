{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn.functional as F\n",
    "from torch import ByteTensor, DoubleTensor, FloatTensor, HalfTensor, LongTensor, ShortTensor, Tensor\n",
    "from torch import nn, optim, as_tensor\n",
    "from torch.utils.data import BatchSampler, DataLoader, Dataset, Sampler, TensorDataset\n",
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "from collections.abc import Iterable\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_train_x = pd.read_csv('data/train_features.csv')\n",
    "ref_train_y = pd.read_csv('data/train_targets_scored.csv')\n",
    "# train_y2 = pd.read_csv('data/train_targets_nonscored.csv')\n",
    "\n",
    "ref_test_x = pd.read_csv('data/test_features.csv')\n",
    "smplsub = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_stratified_val_idxs(df, val_size=0.1, rnd=0):\n",
    "    \n",
    "    arr = df.to_numpy()\n",
    "\n",
    "    X = arr[:,0]\n",
    "    y = arr[:,1:] # this works irrespective of whether labels are space- or comma-separated\n",
    "    \n",
    "    ### sklearn.model_selection.StratifiedKFold\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=rnd)\n",
    "    \n",
    "    for train_index, val_index in sss.split(X, y):\n",
    "        trn_idxs = train_index\n",
    "        val_idxs = val_index\n",
    "\n",
    "    data_report(df, trn_idxs, val_idxs)\n",
    "    return trn_idxs, val_idxs\n",
    "\n",
    "def finalize_df(df, targets, as_multi=True): \n",
    "    # Select and fuse labels into target column (space separated)\n",
    "    df_slct = df[[df.columns[0]] + targets]\n",
    "    if as_multi:\n",
    "        df_out = np.array([[df_slct.values[i][0], ' '.join(str(x) for x in df_slct.values[i][1:])] for i in range(len(df_slct))])\n",
    "        return pd.DataFrame(df_out, columns = [\"ID\", \"Target\"])\n",
    "    else: \n",
    "        df_out = np.array(df_slct)\n",
    "        if len(targets) == 1: return pd.DataFrame(df_out, columns = [\"ID\", 'Target'])\n",
    "        else: return pd.DataFrame(df_out, columns = [\"ID\"] + targets)\n",
    "\n",
    "def data_report(df, trn_idxs, val_idxs, test_csv=None):\n",
    "    trnval = df\n",
    "    if len(trnval.columns) != 2:\n",
    "        print(f\"Multilabel csv with comma-separated labels detected!\\n\")\n",
    "        trnval = finalize_df(trnval, targets=list(trnval.columns)[1:])\n",
    "    print(f\"\"\"Train label-distribution:\\n\"\"\"\n",
    "          f\"\"\"{trnval['Target'][trn_idxs].value_counts()}\\n\"\"\"\n",
    "          f\"\"\"Total: {len(trn_idxs)}\\n\"\"\")\n",
    "    print(f\"\"\"Val label-distribution:\\n\"\"\"\n",
    "          f\"\"\"{trnval['Target'][val_idxs].value_counts()}\\n\"\"\"\n",
    "          f\"\"\"Total: {len(val_idxs)}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilabel csv with comma-separated labels detected!\n",
      "\n",
      "Train label-distribution:\n",
      "trt_cp 48         6842\n",
      "trt_cp 72         6462\n",
      "trt_cp 24         6449\n",
      "ctl_vehicle 48     583\n",
      "ctl_vehicle 72     551\n",
      "ctl_vehicle 24     545\n",
      "Name: Target, dtype: int64\n",
      "Total: 21432\n",
      "\n",
      "Val label-distribution:\n",
      "trt_cp 48         760\n",
      "trt_cp 72         718\n",
      "trt_cp 24         717\n",
      "ctl_vehicle 48     65\n",
      "ctl_vehicle 24     61\n",
      "ctl_vehicle 72     61\n",
      "Name: Target, dtype: int64\n",
      "Total: 2382\n"
     ]
    }
   ],
   "source": [
    "trn_idxs, val_idxs = get_label_stratified_val_idxs(ref_train_x.iloc[:,:3], val_size=0.1, rnd=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert_to_numeric\n",
    "# ref_train_x['cp_type'] = ref_train_x['cp_type'].apply(lambda x : 0 if x == \"trt_cp\" else 1 )\n",
    "# ref_train_x['cp_dose'] = ref_train_x['cp_dose'].apply(lambda x : 1 if x == \"D1\" else 2 )\n",
    "\n",
    "# ref_test_x['cp_type'] = ref_test_x['cp_type'].apply(lambda x : 0 if x == \"trt_cp\" else 1 )\n",
    "# ref_test_x['cp_dose'] = ref_test_x['cp_dose'].apply(lambda x : 1 if x == \"D1\" else 2 )\n",
    "\n",
    "# # split train/val\n",
    "# _X_train, _Y_train = ref_train_x.loc[trn_idxs], ref_train_y.loc[trn_idxs]\n",
    "# _X_valid, _Y_valid = ref_train_x.loc[val_idxs], ref_train_y.loc[val_idxs]\n",
    "\n",
    "# # select features / format dfs:\n",
    "# x_fts = list(_X_train.columns[1:])\n",
    "# y_fts = list(_Y_train.columns[1:])\n",
    "# X_train, X_valid, X_test = _X_train[x_fts], _X_valid[x_fts], ref_test_x[x_fts]\n",
    "# Y_train, Y_valid = _Y_train[y_fts], _Y_valid[y_fts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_col(df, col):\n",
    "    enc = pd.get_dummies(df[col])\n",
    "    enc.columns = [f\"{col}_{n}\" for n in enc.columns]\n",
    "    df = df.drop(col, axis=1)\n",
    "    df = df.join(enc)\n",
    "    return df\n",
    "\n",
    "def prep_data(df, cols, func=onehot_col):\n",
    "    for i in cols: df = func(df, i)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ref_train_x = prep_data(ref_train_x, cols=['cp_type', 'cp_time', 'cp_dose'])\n",
    "_ref_test_x = prep_data(ref_test_x, cols=['cp_type', 'cp_time', 'cp_dose'])\n",
    "\n",
    "x_fts = _ref_train_x.columns[1:]\n",
    "y_fts = ref_train_y.columns[1:]\n",
    "\n",
    "trnval_df_rdy = pd.merge(_ref_train_x, ref_train_y, on='sig_id')\n",
    "test_df_rdy = _ref_test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-1</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>g-6</th>\n",
       "      <th>g-7</th>\n",
       "      <th>g-8</th>\n",
       "      <th>...</th>\n",
       "      <th>c-97</th>\n",
       "      <th>c-98</th>\n",
       "      <th>c-99</th>\n",
       "      <th>cp_type_ctl_vehicle</th>\n",
       "      <th>cp_type_trt_cp</th>\n",
       "      <th>cp_time_24</th>\n",
       "      <th>cp_time_48</th>\n",
       "      <th>cp_time_72</th>\n",
       "      <th>cp_dose_D1</th>\n",
       "      <th>cp_dose_D2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_0004d9e33</td>\n",
       "      <td>-0.5458</td>\n",
       "      <td>0.1306</td>\n",
       "      <td>-0.5135</td>\n",
       "      <td>0.4408</td>\n",
       "      <td>1.5500</td>\n",
       "      <td>-0.1644</td>\n",
       "      <td>-0.2140</td>\n",
       "      <td>0.2221</td>\n",
       "      <td>-0.3260</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0502</td>\n",
       "      <td>0.1510</td>\n",
       "      <td>-0.7750</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_001897cda</td>\n",
       "      <td>-0.1829</td>\n",
       "      <td>0.2320</td>\n",
       "      <td>1.2080</td>\n",
       "      <td>-0.4522</td>\n",
       "      <td>-0.3652</td>\n",
       "      <td>-0.3319</td>\n",
       "      <td>-1.8820</td>\n",
       "      <td>0.4022</td>\n",
       "      <td>-0.3528</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4764</td>\n",
       "      <td>-1.3810</td>\n",
       "      <td>-0.7300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_002429b5b</td>\n",
       "      <td>0.1852</td>\n",
       "      <td>-0.1404</td>\n",
       "      <td>-0.3911</td>\n",
       "      <td>0.1310</td>\n",
       "      <td>-1.4380</td>\n",
       "      <td>0.2455</td>\n",
       "      <td>-0.3390</td>\n",
       "      <td>-0.3206</td>\n",
       "      <td>0.6944</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0160</td>\n",
       "      <td>0.4924</td>\n",
       "      <td>-0.1942</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00276f245</td>\n",
       "      <td>0.4828</td>\n",
       "      <td>0.1955</td>\n",
       "      <td>0.3825</td>\n",
       "      <td>0.4244</td>\n",
       "      <td>-0.5855</td>\n",
       "      <td>-1.2020</td>\n",
       "      <td>0.5998</td>\n",
       "      <td>-0.1799</td>\n",
       "      <td>0.9365</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1305</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>-0.5809</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_0027f1083</td>\n",
       "      <td>-0.3979</td>\n",
       "      <td>-1.2680</td>\n",
       "      <td>1.9130</td>\n",
       "      <td>0.2057</td>\n",
       "      <td>-0.5864</td>\n",
       "      <td>-0.0166</td>\n",
       "      <td>0.5128</td>\n",
       "      <td>0.6365</td>\n",
       "      <td>0.2611</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5313</td>\n",
       "      <td>0.9931</td>\n",
       "      <td>1.8380</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 880 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sig_id     g-0     g-1     g-2     g-3     g-4     g-5     g-6  \\\n",
       "0  id_0004d9e33 -0.5458  0.1306 -0.5135  0.4408  1.5500 -0.1644 -0.2140   \n",
       "1  id_001897cda -0.1829  0.2320  1.2080 -0.4522 -0.3652 -0.3319 -1.8820   \n",
       "2  id_002429b5b  0.1852 -0.1404 -0.3911  0.1310 -1.4380  0.2455 -0.3390   \n",
       "3  id_00276f245  0.4828  0.1955  0.3825  0.4244 -0.5855 -1.2020  0.5998   \n",
       "4  id_0027f1083 -0.3979 -1.2680  1.9130  0.2057 -0.5864 -0.0166  0.5128   \n",
       "\n",
       "      g-7     g-8  ...    c-97    c-98    c-99  cp_type_ctl_vehicle  \\\n",
       "0  0.2221 -0.3260  ... -0.0502  0.1510 -0.7750                    0   \n",
       "1  0.4022 -0.3528  ... -0.4764 -1.3810 -0.7300                    0   \n",
       "2 -0.3206  0.6944  ...  1.0160  0.4924 -0.1942                    1   \n",
       "3 -0.1799  0.9365  ... -0.1305  0.5645 -0.5809                    0   \n",
       "4  0.6365  0.2611  ... -0.5313  0.9931  1.8380                    0   \n",
       "\n",
       "   cp_type_trt_cp  cp_time_24  cp_time_48  cp_time_72  cp_dose_D1  cp_dose_D2  \n",
       "0               1           1           0           0           1           0  \n",
       "1               1           0           0           1           1           0  \n",
       "2               0           1           0           0           1           0  \n",
       "3               1           1           0           0           0           1  \n",
       "4               1           0           1           0           1           0  \n",
       "\n",
       "[5 rows x 880 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_rdy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MOA_data:\n",
    "    def __init__(self, x_fts, y_fts, bs=512):\n",
    "        self.x_fts, self.y_fts, self.bs = x_fts, y_fts, bs\n",
    "    \n",
    "    def embed(df):\n",
    "        return df\n",
    "\n",
    "    def create(self, df, val_idxs, test=None): \n",
    "        train = df.drop(val_idxs)\n",
    "        valid = df.loc[val_idxs]\n",
    "        for ID in val['sig_id']: assert ID not in list(trn['sig_id']) \n",
    "        self.train_ds = MOA_ds(train, self.x_fts, self.y_fts)\n",
    "        self.valid_ds = MOA_ds(valid, self.x_fts, self.y_fts)\n",
    "        self.train_dl = DataLoader(self.train_ds, batch_size=self.bs, shuffle=True)\n",
    "        self.valid_dl = DataLoader(self.valid_ds, batch_size=self.bs, shuffle=False)\n",
    "        \n",
    "        if test is not None:\n",
    "            self.test_ds = MOA_ds(test, self.x_fts, y_fts, test=True)\n",
    "            self.test_dl = DataLoader(self.test_ds, batch_size=self.bs, shuffle=False)\n",
    "\n",
    "class MOA_ds(Dataset):\n",
    "    def __init__(self, df, x_fts, y_fts, test=False):\n",
    "        if test: self.x, self.y = df[x_fts].to_numpy(), np.zeros((df.shape[0], len(y_fts)))\n",
    "        else: self.x, self.y = df[x_fts].to_numpy(), df[y_fts].to_numpy()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return [torch.tensor(self.x[idx, :], dtype=torch.float),\n",
    "                torch.tensor(self.y[idx, :], dtype=torch.float)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MOA_data(x_fts, y_fts)\n",
    "data.create(trnval_df_rdy, val_idxs, test=test_df_rdy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ifnone(a,b):\n",
    "    \"`a` if `a` is not None, otherwise `b`.\"\n",
    "    return b if a is None else a\n",
    "\n",
    "def listify(p=None, q=None):\n",
    "    \"Make `p` listy and the same length as `q`.\"\n",
    "    if p is None: p=[]\n",
    "    elif isinstance(p, str):          p = [p]\n",
    "    elif not isinstance(p, Iterable): p = [p]\n",
    "    #Rank 0 tensors in PyTorch are Iterable but don't have a length.\n",
    "    else:\n",
    "        try: a = len(p)\n",
    "        except: p = [p]\n",
    "    n = q if type(q)==int else len(p) if q is None else len(q)\n",
    "    if len(p)==1: p = p * n\n",
    "    assert len(p)==n, f'List len mismatch ({len(p)} vs {n})'\n",
    "    return list(p)\n",
    "\n",
    "def emb_sz_rule(n_cat:int)->int: return min(600, round(1.6 * n_cat**0.56))\n",
    "\n",
    "def def_emb_sz(classes, n, sz_dict=None):\n",
    "    \"Pick an embedding size for `n` depending on `classes` if not given in `sz_dict`.\"\n",
    "    sz_dict = ifnone(sz_dict, {})\n",
    "    n_cat = len(classes[n])\n",
    "    sz = sz_dict.get(n, int(emb_sz_rule(n_cat)))  # rule of thumb\n",
    "    return n_cat,sz\n",
    "\n",
    "def get_emb_szs(self, sz_dict=None):\n",
    "    \"Return the default embedding sizes suitable for this data or takes the ones in `sz_dict`.\"\n",
    "    return [def_emb_sz(self.classes, n, sz_dict) for n in self.cat_names]\n",
    "\n",
    "def embedding(ni,nf):\n",
    "    \"Create an embedding layer.\"\n",
    "    emb = nn.Embedding(ni, nf)\n",
    "    with torch.no_grad(): trunc_normal_(emb.weight, std=0.01)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_drop_lin(n_in, n_out, bn=True, p=0., actn=None):\n",
    "    \"Sequence of batchnorm (if `bn`), dropout (with `p`) and linear (`n_in`,`n_out`) layers followed by `actn`.\"\n",
    "    layers = [nn.BatchNorm1d(n_in)] if bn else []\n",
    "    if p != 0: layers.append(nn.Dropout(p))\n",
    "    layers.append(nn.Linear(n_in, n_out))\n",
    "    if actn is not None: layers.append(actn)\n",
    "    return layers\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, in_fts, layers, out_sz, ps=None, use_bn=True, bn_final=False):\n",
    "        super().__init__()\n",
    "        ps = ifnone(ps, [0]*len(layers))\n",
    "        ps = listify(ps, layers)\n",
    "        sizes = [in_fts] + layers + [out_sz]\n",
    "        actns = [nn.ReLU(inplace=True) for _ in range(len(sizes)-2)] + [None]\n",
    "        layers = []\n",
    "        \n",
    "        for i,(n_in,n_out,dp,act) in enumerate(zip(sizes[:-1],sizes[1:],[0.]+ps,actns)):\n",
    "            layers += bn_drop_lin(n_in, n_out, bn=use_bn and i!=0, p=dp, actn=act)\n",
    "        if bn_final: layers.append(nn.BatchNorm1d(sizes[-1]))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNet(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=879, out_features=512, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.3)\n",
       "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.3)\n",
       "    (8): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (9): ReLU(inplace)\n",
       "    (10): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Dropout(p=0.3)\n",
       "    (12): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (13): ReLU(inplace)\n",
       "    (14): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (15): Dropout(p=0.3)\n",
       "    (16): Linear(in_features=128, out_features=206, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ni = data.train_ds.x.shape[1]\n",
    "layers = [512, 512, 256, 128]\n",
    "out_sz = 206\n",
    "\n",
    "m = SimpleNet(ni, layers, out_sz, ps=0.3)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_np(x): \n",
    "    return x.data.cpu().numpy()\n",
    "\n",
    "def loss_batch(model, x, y, loss_func, opt=None): \n",
    "    out = model(x)\n",
    "    if not loss_func: return to_np(out), to_np(y)\n",
    "    loss = loss_func(out, y)\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    return loss.detach().cpu()    \n",
    "    \n",
    "def validate(model, dl, loss_fn=None, average=True):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses,nums = [],[]\n",
    "#         for xb,yb in tqdm(dl, total=len(dl), unit='batches'):\n",
    "        for xb,yb in dl:\n",
    "            val_loss = loss_batch(model, xb, yb, loss_fn)\n",
    "            val_losses.append(val_loss)\n",
    "            nums.append(xb.shape[0])\n",
    "        nums = np.array(nums, dtype=np.float32)\n",
    "        if average: return (to_np(torch.stack(val_losses)) * nums).sum() / nums.sum()\n",
    "        else:       return val_losses\n",
    "        \n",
    "def fit(model, data, loss_fn, opt, epochs, average=True):\n",
    "    for e in tqdm(range(epochs), total=epochs, unit='epochs'):\n",
    "        model.train()\n",
    "        train_losses, nums = [], []\n",
    "#         for xb,yb in tqdm(data.train_dl, total=len(data.train_dl), unit='batches'):\n",
    "        for xb,yb in data.train_dl:\n",
    "            loss = loss_batch(model, xb, yb, loss_fn, opt)\n",
    "            train_losses.append(loss)\n",
    "            nums.append(xb.shape[0])\n",
    "        nums = np.array(nums, dtype=np.float32)\n",
    "        train_loss = (np.stack(train_losses) * nums).sum() / nums.sum()\n",
    "        valid_loss = validate(model, data.valid_dl, loss_fn, average=True)\n",
    "        print(f\"Epoch {e} -- train_loss: {train_loss}, valid_loss: {valid_loss}\")\n",
    "    print('done!')\n",
    "\n",
    "class learner():\n",
    "    def __init__(self, model, data, loss_fn, opt=optim.Adam):\n",
    "        self.m, self.data, self.loss_fn = model, data, loss_fn\n",
    "        self.opt = opt\n",
    "    \n",
    "    def fit(self, epochs, lr=1e-3, wd=1e-5):\n",
    "        opt = self.opt(self.m.parameters(), lr=lr)\n",
    "        fit(self.m, self.data, self.loss_fn, opt, epochs)  \n",
    "        \n",
    "    def predict(self, dl):\n",
    "        return validate(self.m, dl, loss_fn=None, average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "ni = data.train_ds.x.shape[1]\n",
    "layers = [512, 512, 256, 128]\n",
    "out_sz = 206\n",
    "\n",
    "m = SimpleNet(ni, layers, out_sz, ps=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "learn = learner(m, data, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371bac1f9e4b4a29877185553536cc55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 -- train_loss: 0.20166140794754028, valid_loss: 0.022816205397248268\n",
      "Epoch 1 -- train_loss: 0.021588550880551338, valid_loss: 0.01898844540119171\n",
      "Epoch 2 -- train_loss: 0.0195272509008646, valid_loss: 0.018195243552327156\n",
      "Epoch 3 -- train_loss: 0.01900353468954563, valid_loss: 0.017815884202718735\n",
      "Epoch 4 -- train_loss: 0.01858995296061039, valid_loss: 0.01741376519203186\n",
      "Epoch 5 -- train_loss: 0.018144868314266205, valid_loss: 0.016912614926695824\n",
      "Epoch 6 -- train_loss: 0.017706310376524925, valid_loss: 0.016627350822091103\n",
      "Epoch 7 -- train_loss: 0.01733754761517048, valid_loss: 0.0163926612585783\n",
      "Epoch 8 -- train_loss: 0.017133638262748718, valid_loss: 0.01624292880296707\n",
      "Epoch 9 -- train_loss: 0.016882838681340218, valid_loss: 0.016110436990857124\n",
      "Epoch 10 -- train_loss: 0.01664646714925766, valid_loss: 0.015941431745886803\n",
      "Epoch 11 -- train_loss: 0.01645747572183609, valid_loss: 0.015824882313609123\n",
      "Epoch 12 -- train_loss: 0.01623309962451458, valid_loss: 0.015744144096970558\n",
      "Epoch 13 -- train_loss: 0.016132021322846413, valid_loss: 0.015633808448910713\n",
      "Epoch 14 -- train_loss: 0.015900280326604843, valid_loss: 0.015614586882293224\n",
      "Epoch 15 -- train_loss: 0.015773087739944458, valid_loss: 0.015542295761406422\n",
      "Epoch 16 -- train_loss: 0.015570426359772682, valid_loss: 0.015590384602546692\n",
      "Epoch 17 -- train_loss: 0.015443851239979267, valid_loss: 0.015526950359344482\n",
      "Epoch 18 -- train_loss: 0.015245042741298676, valid_loss: 0.015483573079109192\n",
      "Epoch 19 -- train_loss: 0.015104195103049278, valid_loss: 0.015554250217974186\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "learn.fit(20, lr=1e-2, wd=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(res_list):\n",
    "    preds = np.vstack([p[0] for p in res_list])\n",
    "    preds = nn.Sigmoid()(torch.tensor(preds))\n",
    "    y = np.vstack([p[1] for p in res_list])\n",
    "    return [preds, y]\n",
    "\n",
    "def eval_model(learn_obj):\n",
    "    res = {'train preds': unpack(learn_obj.predict(learn_obj.data.train_dl)), \n",
    "           'valid preds': unpack(learn_obj.predict(learn_obj.data.valid_dl)), \n",
    "           'train baseline': [learn_obj.data.train_ds.y, np.zeros(learn_obj.data.train_ds.y.shape)],\n",
    "           'valid baseline': [learn_obj.data.valid_ds.y, np.zeros(learn_obj.data.valid_ds.y.shape)]}\n",
    "        \n",
    "    return res['train preds'], res['valid preds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_res, valid_res = eval_model(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = to_np(train_res[0])\n",
    "train_y = train_res[1]\n",
    "\n",
    "valid_pred = to_np(valid_res[0])\n",
    "valid_y = valid_res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4399802.,       0.,       0.,       0.,       0.,       0.,\n",
       "              0.,       0.,       0.,   15190.]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ],\n",
       "       dtype=float32),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(train_y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1842602658996806\n",
      "3.7761496362408424\n",
      "\n",
      "\n",
      "2.4985078247321804\n",
      "3.699541218798475\n"
     ]
    }
   ],
   "source": [
    "print(metrics.log_loss(train_y, train_pred))\n",
    "print(metrics.log_loss(train_y, np.zeros(train_y.shape)))\n",
    "print('\\n')\n",
    "print(metrics.log_loss(valid_y, valid_pred))\n",
    "print(metrics.log_loss(valid_y, np.zeros(valid_y.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
