{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch, torch.nn.functional as F\n",
    "from torch import ByteTensor, DoubleTensor, FloatTensor, HalfTensor, LongTensor, ShortTensor, Tensor\n",
    "from torch import nn, optim, as_tensor\n",
    "from torch.utils.data import BatchSampler, DataLoader, Dataset, Sampler, TensorDataset\n",
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "from collections.abc import Iterable\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_train_x = pd.read_csv('data/train_features.csv')\n",
    "ref_train_y = pd.read_csv('data/train_targets_scored.csv')\n",
    "ref_train_y2 = pd.read_csv('data/train_targets_nonscored.csv')\n",
    "\n",
    "ref_test_x = pd.read_csv('data/test_features.csv')\n",
    "smplsub = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_all = pd.merge(ref_train_y, ref_train_y2, on='sig_id')\n",
    "train_y_all['ctrl'] = (ref_train_x['cp_type'] == 'ctl_vehicle').astype(int)\n",
    "train_y_all['other'] = (train_y_all.loc[:,ref_train_y2.columns].sum(axis=1) > 0).astype(int)\n",
    "train_y_all['zero'] = (train_y_all.sum(axis=1) == 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_train_y = train_y_all.loc[:,list(ref_train_y.columns)+['ctrl', 'zero', 'other']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sig_id                                             id_000644bb2id_000779bfcid_000a6266aid_0015fd3...\n",
       "5-alpha_reductase_inhibitor                                                                       17\n",
       "11-beta-hsd1_inhibitor                                                                            18\n",
       "acat_inhibitor                                                                                    24\n",
       "acetylcholine_receptor_agonist                                                                   190\n",
       "acetylcholine_receptor_antagonist                                                                301\n",
       "acetylcholinesterase_inhibitor                                                                    73\n",
       "adenosine_receptor_agonist                                                                        54\n",
       "adenosine_receptor_antagonist                                                                     96\n",
       "adenylyl_cyclase_activator                                                                        12\n",
       "adrenergic_receptor_agonist                                                                      270\n",
       "adrenergic_receptor_antagonist                                                                   360\n",
       "akt_inhibitor                                                                                     66\n",
       "aldehyde_dehydrogenase_inhibitor                                                                   7\n",
       "alk_inhibitor                                                                                     42\n",
       "ampk_activator                                                                                    12\n",
       "analgesic                                                                                         12\n",
       "androgen_receptor_agonist                                                                         48\n",
       "androgen_receptor_antagonist                                                                      89\n",
       "anesthetic_-_local                                                                                80\n",
       "angiogenesis_inhibitor                                                                            36\n",
       "angiotensin_receptor_antagonist                                                                   37\n",
       "anti-inflammatory                                                                                 73\n",
       "antiarrhythmic                                                                                     6\n",
       "antibiotic                                                                                        43\n",
       "anticonvulsant                                                                                    12\n",
       "antifungal                                                                                        13\n",
       "antihistamine                                                                                     12\n",
       "antimalarial                                                                                      18\n",
       "antioxidant                                                                                       73\n",
       "                                                                         ...                        \n",
       "sigma_receptor_agonist                                                                            36\n",
       "sigma_receptor_antagonist                                                                         36\n",
       "smoothened_receptor_antagonist                                                                    25\n",
       "sodium_channel_inhibitor                                                                         267\n",
       "sphingosine_receptor_agonist                                                                      25\n",
       "src_inhibitor                                                                                     71\n",
       "steroid                                                                                            6\n",
       "syk_inhibitor                                                                                     19\n",
       "tachykinin_antagonist                                                                             60\n",
       "tgf-beta_receptor_inhibitor                                                                       30\n",
       "thrombin_inhibitor                                                                                19\n",
       "thymidylate_synthase_inhibitor                                                                    37\n",
       "tlr_agonist                                                                                       30\n",
       "tlr_antagonist                                                                                     7\n",
       "tnf_inhibitor                                                                                     36\n",
       "topoisomerase_inhibitor                                                                          127\n",
       "transient_receptor_potential_channel_antagonist                                                   18\n",
       "tropomyosin_receptor_kinase_inhibitor                                                              6\n",
       "trpv_agonist                                                                                      25\n",
       "trpv_antagonist                                                                                   48\n",
       "tubulin_inhibitor                                                                                316\n",
       "tyrosine_kinase_inhibitor                                                                         73\n",
       "ubiquitin_specific_protease_inhibitor                                                              6\n",
       "vegfr_inhibitor                                                                                  170\n",
       "vitamin_b                                                                                         26\n",
       "vitamin_d_receptor_agonist                                                                        39\n",
       "wnt_inhibitor                                                                                     30\n",
       "ctrl                                                                                            1866\n",
       "zero                                                                                            3664\n",
       "other                                                                                           4590\n",
       "Length: 210, dtype: object"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_train_y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_stratified_val_idxs(df, val_size=0.1, rnd=0):\n",
    "    \n",
    "    arr = df.to_numpy()\n",
    "\n",
    "    X = arr[:,0]\n",
    "    y = arr[:,1:] # this works irrespective of whether labels are space- or comma-separated\n",
    "    \n",
    "    ### sklearn.model_selection.StratifiedKFold\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=rnd)\n",
    "    \n",
    "    for train_index, val_index in sss.split(X, y):\n",
    "        trn_idxs = train_index\n",
    "        val_idxs = val_index\n",
    "\n",
    "    data_report(df, trn_idxs, val_idxs)\n",
    "    return trn_idxs, val_idxs\n",
    "\n",
    "def finalize_df(df, targets, as_multi=True): \n",
    "    # Select and fuse labels into target column (space separated)\n",
    "    df_slct = df[[df.columns[0]] + targets]\n",
    "    if as_multi:\n",
    "        df_out = np.array([[df_slct.values[i][0], ' '.join(str(x) for x in df_slct.values[i][1:])] for i in range(len(df_slct))])\n",
    "        return pd.DataFrame(df_out, columns = [\"ID\", \"Target\"])\n",
    "    else: \n",
    "        df_out = np.array(df_slct)\n",
    "        if len(targets) == 1: return pd.DataFrame(df_out, columns = [\"ID\", 'Target'])\n",
    "        else: return pd.DataFrame(df_out, columns = [\"ID\"] + targets)\n",
    "\n",
    "def data_report(df, trn_idxs, val_idxs, test_csv=None):\n",
    "    trnval = df\n",
    "    if len(trnval.columns) != 2:\n",
    "        print(f\"Multilabel csv with comma-separated labels detected!\\n\")\n",
    "        trnval = finalize_df(trnval, targets=list(trnval.columns)[1:])\n",
    "    print(f\"\"\"Train label-distribution:\\n\"\"\"\n",
    "          f\"\"\"{trnval['Target'][trn_idxs].value_counts()}\\n\"\"\"\n",
    "          f\"\"\"Total: {len(trn_idxs)}\\n\"\"\")\n",
    "    print(f\"\"\"Val label-distribution:\\n\"\"\"\n",
    "          f\"\"\"{trnval['Target'][val_idxs].value_counts()}\\n\"\"\"\n",
    "          f\"\"\"Total: {len(val_idxs)}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilabel csv with comma-separated labels detected!\n",
      "\n",
      "Train label-distribution:\n",
      "trt_cp 48 D1         3610\n",
      "trt_cp 72 D1         3240\n",
      "trt_cp 48 D2         3232\n",
      "trt_cp 24 D1         3226\n",
      "trt_cp 24 D2         3223\n",
      "trt_cp 72 D2         3222\n",
      "ctl_vehicle 48 D1     309\n",
      "ctl_vehicle 72 D1     276\n",
      "ctl_vehicle 72 D2     275\n",
      "ctl_vehicle 24 D2     274\n",
      "ctl_vehicle 48 D2     274\n",
      "ctl_vehicle 24 D1     271\n",
      "Name: Target, dtype: int64\n",
      "Total: 21432\n",
      "\n",
      "Val label-distribution:\n",
      "trt_cp 48 D1         401\n",
      "trt_cp 72 D1         360\n",
      "trt_cp 48 D2         359\n",
      "trt_cp 24 D1         359\n",
      "trt_cp 72 D2         358\n",
      "trt_cp 24 D2         358\n",
      "ctl_vehicle 48 D1     34\n",
      "ctl_vehicle 24 D2     31\n",
      "ctl_vehicle 72 D1     31\n",
      "ctl_vehicle 48 D2     31\n",
      "ctl_vehicle 24 D1     30\n",
      "ctl_vehicle 72 D2     30\n",
      "Name: Target, dtype: int64\n",
      "Total: 2382\n"
     ]
    }
   ],
   "source": [
    "trn_idxs, val_idxs = get_label_stratified_val_idxs(ref_train_x.iloc[:,:4], val_size=0.1, rnd=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_col(df, col):\n",
    "    enc = pd.get_dummies(df[col])\n",
    "    enc.columns = [f\"{col}_{n}\" for n in enc.columns]\n",
    "    df = df.drop(col, axis=1)\n",
    "    df = df.join(enc)\n",
    "    return df\n",
    "\n",
    "def prep_data(df, cols, func=onehot_col):\n",
    "    for i in cols: df = func(df, i)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ref_train_x = prep_data(ref_train_x, cols=['cp_type', 'cp_time', 'cp_dose'])\n",
    "_ref_test_x = prep_data(ref_test_x, cols=['cp_type', 'cp_time', 'cp_dose'])\n",
    "\n",
    "x_fts = _ref_train_x.columns[1:]\n",
    "y_fts = ref_train_y.columns[1:]\n",
    "\n",
    "trnval_df_rdy = pd.merge(_ref_train_x, ref_train_y, on='sig_id')\n",
    "test_df_rdy = _ref_test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-1</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>g-6</th>\n",
       "      <th>g-7</th>\n",
       "      <th>g-8</th>\n",
       "      <th>...</th>\n",
       "      <th>c-97</th>\n",
       "      <th>c-98</th>\n",
       "      <th>c-99</th>\n",
       "      <th>cp_type_ctl_vehicle</th>\n",
       "      <th>cp_type_trt_cp</th>\n",
       "      <th>cp_time_24</th>\n",
       "      <th>cp_time_48</th>\n",
       "      <th>cp_time_72</th>\n",
       "      <th>cp_dose_D1</th>\n",
       "      <th>cp_dose_D2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_0004d9e33</td>\n",
       "      <td>-0.5458</td>\n",
       "      <td>0.1306</td>\n",
       "      <td>-0.5135</td>\n",
       "      <td>0.4408</td>\n",
       "      <td>1.5500</td>\n",
       "      <td>-0.1644</td>\n",
       "      <td>-0.2140</td>\n",
       "      <td>0.2221</td>\n",
       "      <td>-0.3260</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0502</td>\n",
       "      <td>0.1510</td>\n",
       "      <td>-0.7750</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_001897cda</td>\n",
       "      <td>-0.1829</td>\n",
       "      <td>0.2320</td>\n",
       "      <td>1.2080</td>\n",
       "      <td>-0.4522</td>\n",
       "      <td>-0.3652</td>\n",
       "      <td>-0.3319</td>\n",
       "      <td>-1.8820</td>\n",
       "      <td>0.4022</td>\n",
       "      <td>-0.3528</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4764</td>\n",
       "      <td>-1.3810</td>\n",
       "      <td>-0.7300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_002429b5b</td>\n",
       "      <td>0.1852</td>\n",
       "      <td>-0.1404</td>\n",
       "      <td>-0.3911</td>\n",
       "      <td>0.1310</td>\n",
       "      <td>-1.4380</td>\n",
       "      <td>0.2455</td>\n",
       "      <td>-0.3390</td>\n",
       "      <td>-0.3206</td>\n",
       "      <td>0.6944</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0160</td>\n",
       "      <td>0.4924</td>\n",
       "      <td>-0.1942</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00276f245</td>\n",
       "      <td>0.4828</td>\n",
       "      <td>0.1955</td>\n",
       "      <td>0.3825</td>\n",
       "      <td>0.4244</td>\n",
       "      <td>-0.5855</td>\n",
       "      <td>-1.2020</td>\n",
       "      <td>0.5998</td>\n",
       "      <td>-0.1799</td>\n",
       "      <td>0.9365</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1305</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>-0.5809</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_0027f1083</td>\n",
       "      <td>-0.3979</td>\n",
       "      <td>-1.2680</td>\n",
       "      <td>1.9130</td>\n",
       "      <td>0.2057</td>\n",
       "      <td>-0.5864</td>\n",
       "      <td>-0.0166</td>\n",
       "      <td>0.5128</td>\n",
       "      <td>0.6365</td>\n",
       "      <td>0.2611</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5313</td>\n",
       "      <td>0.9931</td>\n",
       "      <td>1.8380</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 880 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sig_id     g-0     g-1     g-2     g-3     g-4     g-5     g-6  \\\n",
       "0  id_0004d9e33 -0.5458  0.1306 -0.5135  0.4408  1.5500 -0.1644 -0.2140   \n",
       "1  id_001897cda -0.1829  0.2320  1.2080 -0.4522 -0.3652 -0.3319 -1.8820   \n",
       "2  id_002429b5b  0.1852 -0.1404 -0.3911  0.1310 -1.4380  0.2455 -0.3390   \n",
       "3  id_00276f245  0.4828  0.1955  0.3825  0.4244 -0.5855 -1.2020  0.5998   \n",
       "4  id_0027f1083 -0.3979 -1.2680  1.9130  0.2057 -0.5864 -0.0166  0.5128   \n",
       "\n",
       "      g-7     g-8  ...    c-97    c-98    c-99  cp_type_ctl_vehicle  \\\n",
       "0  0.2221 -0.3260  ... -0.0502  0.1510 -0.7750                    0   \n",
       "1  0.4022 -0.3528  ... -0.4764 -1.3810 -0.7300                    0   \n",
       "2 -0.3206  0.6944  ...  1.0160  0.4924 -0.1942                    1   \n",
       "3 -0.1799  0.9365  ... -0.1305  0.5645 -0.5809                    0   \n",
       "4  0.6365  0.2611  ... -0.5313  0.9931  1.8380                    0   \n",
       "\n",
       "   cp_type_trt_cp  cp_time_24  cp_time_48  cp_time_72  cp_dose_D1  cp_dose_D2  \n",
       "0               1           1           0           0           1           0  \n",
       "1               1           0           0           1           1           0  \n",
       "2               0           1           0           0           1           0  \n",
       "3               1           1           0           0           0           1  \n",
       "4               1           0           1           0           1           0  \n",
       "\n",
       "[5 rows x 880 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_rdy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MOA_data:\n",
    "    def __init__(self, x_fts, y_fts, bs=512):\n",
    "        self.x_fts, self.y_fts, self.bs = x_fts, y_fts, bs\n",
    "    \n",
    "    def embed(df):\n",
    "        return df\n",
    "    \n",
    "    def get_weights(targ_df):\n",
    "        cls_weight = ((df.sum(axis=0) / len(df)).to_numpy())\n",
    "        res = df.to_numpy() * cls_weight\n",
    "        weights = res.sum(axis=1)\n",
    "        assert (weights == 0).any()\n",
    "        return weights\n",
    "\n",
    "    def create(self, df, val_idxs, test=None, sampler=None): \n",
    "        train = df.drop(val_idxs)\n",
    "        valid = df.loc[val_idxs]\n",
    "        for ID in valid['sig_id']: assert ID not in list(train['sig_id']) \n",
    "        self.train_ds = MOA_ds(train, self.x_fts, self.y_fts)\n",
    "        self.valid_ds = MOA_ds(valid, self.x_fts, self.y_fts)\n",
    "        \n",
    "        self.train_dl = DataLoader(self.train_ds, batch_size=self.bs, shuffle=True, sampler=sampler)\n",
    "        self.valid_dl = DataLoader(self.valid_ds, batch_size=self.bs, shuffle=False)\n",
    "        self.fix_dl = DataLoader(self.train_ds, batch_size=self.bs, shuffle=False)\n",
    "        \n",
    "        if test is not None:\n",
    "            self.test_ds = MOA_ds(test, self.x_fts, y_fts, test=True)\n",
    "            self.test_dl = DataLoader(self.test_ds, batch_size=self.bs, shuffle=False)\n",
    "\n",
    "class MOA_ds(Dataset):\n",
    "    def __init__(self, df, x_fts, y_fts, test=False):\n",
    "        if test: self.x, self.y = df[x_fts].to_numpy(), np.zeros((df.shape[0], len(y_fts)))\n",
    "        else: self.x, self.y = df[x_fts].to_numpy(), df[y_fts].to_numpy()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return [torch.tensor(self.x[idx, :], dtype=torch.float),\n",
    "                torch.tensor(self.y[idx, :], dtype=torch.float)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MOA_data(x_fts, y_fts)\n",
    "data.create(trnval_df_rdy, val_idxs, test=test_df_rdy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ifnone(a,b):\n",
    "    \"`a` if `a` is not None, otherwise `b`.\"\n",
    "    return b if a is None else a\n",
    "\n",
    "def listify(p=None, q=None):\n",
    "    \"Make `p` listy and the same length as `q`.\"\n",
    "    if p is None: p=[]\n",
    "    elif isinstance(p, str):          p = [p]\n",
    "    elif not isinstance(p, Iterable): p = [p]\n",
    "    #Rank 0 tensors in PyTorch are Iterable but don't have a length.\n",
    "    else:\n",
    "        try: a = len(p)\n",
    "        except: p = [p]\n",
    "    n = q if type(q)==int else len(p) if q is None else len(q)\n",
    "    if len(p)==1: p = p * n\n",
    "    assert len(p)==n, f'List len mismatch ({len(p)} vs {n})'\n",
    "    return list(p)\n",
    "\n",
    "def emb_sz_rule(n_cat:int)->int: return min(600, round(1.6 * n_cat**0.56))\n",
    "\n",
    "def def_emb_sz(classes, n, sz_dict=None):\n",
    "    \"Pick an embedding size for `n` depending on `classes` if not given in `sz_dict`.\"\n",
    "    sz_dict = ifnone(sz_dict, {})\n",
    "    n_cat = len(classes[n])\n",
    "    sz = sz_dict.get(n, int(emb_sz_rule(n_cat)))  # rule of thumb\n",
    "    return n_cat,sz\n",
    "\n",
    "def get_emb_szs(self, sz_dict=None):\n",
    "    \"Return the default embedding sizes suitable for this data or takes the ones in `sz_dict`.\"\n",
    "    return [def_emb_sz(self.classes, n, sz_dict) for n in self.cat_names]\n",
    "\n",
    "def embedding(ni,nf):\n",
    "    \"Create an embedding layer.\"\n",
    "    emb = nn.Embedding(ni, nf)\n",
    "    with torch.no_grad(): trunc_normal_(emb.weight, std=0.01)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_drop_lin(n_in, n_out, bn=True, p=0., actn=None):\n",
    "    \"Sequence of batchnorm (if `bn`), dropout (with `p`) and linear (`n_in`,`n_out`) layers followed by `actn`.\"\n",
    "    layers = [nn.BatchNorm1d(n_in)] if bn else []\n",
    "    if p != 0: layers.append(nn.Dropout(p))\n",
    "    layers.append(nn.Linear(n_in, n_out))\n",
    "    if actn is not None: layers.append(actn)\n",
    "    return layers\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, in_fts, layers, out_sz, ps=None, use_bn=True, bn_final=False):\n",
    "        super().__init__()\n",
    "        ps = ifnone(ps, [0]*len(layers))\n",
    "        ps = listify(ps, layers)\n",
    "        sizes = [in_fts] + layers + [out_sz]\n",
    "        actns = [nn.ReLU(inplace=True) for _ in range(len(sizes)-2)] + [None]\n",
    "        layers = []\n",
    "        \n",
    "        for i,(n_in,n_out,dp,act) in enumerate(zip(sizes[:-1],sizes[1:],[0.]+ps,actns)):\n",
    "            layers += bn_drop_lin(n_in, n_out, bn=use_bn and i!=0, p=dp, actn=act)\n",
    "        if bn_final: layers.append(nn.BatchNorm1d(sizes[-1]))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNet(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=879, out_features=512, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.3)\n",
       "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.3)\n",
       "    (8): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (9): ReLU(inplace)\n",
       "    (10): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Dropout(p=0.3)\n",
       "    (12): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (13): ReLU(inplace)\n",
       "    (14): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (15): Dropout(p=0.3)\n",
       "    (16): Linear(in_features=128, out_features=206, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ni = data.train_ds.x.shape[1]\n",
    "layers = [512, 512, 256, 128]\n",
    "out_sz = 206\n",
    "\n",
    "m = SimpleNet(ni, layers, out_sz, ps=0.3)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_np(x): \n",
    "    return x.data.cpu().numpy()\n",
    "\n",
    "def loss_batch(model, x, y, loss_func, opt=None): \n",
    "    out = model(x)\n",
    "    if not loss_func: return to_np(out), to_np(y)\n",
    "    loss = loss_func(out, y)\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    return loss.detach().cpu()    \n",
    "    \n",
    "def validate(model, dl, loss_fn=None, average=True):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses,nums = [],[]\n",
    "#         for xb,yb in tqdm(dl, total=len(dl), unit='batches'):\n",
    "        for xb,yb in dl:\n",
    "            val_loss = loss_batch(model, xb, yb, loss_fn)\n",
    "            val_losses.append(val_loss)\n",
    "            nums.append(xb.shape[0])\n",
    "        nums = np.array(nums, dtype=np.float32)\n",
    "        if average: return (to_np(torch.stack(val_losses)) * nums).sum() / nums.sum()\n",
    "        else:       return val_losses\n",
    "        \n",
    "def fit(model, data, loss_fn, opt, epochs, average=True, scd=None):\n",
    "    nb = 0\n",
    "    b2 = opt.param_groups[0]['betas'][1]\n",
    "    for e in tqdm(range(epochs), total=epochs, unit='epochs'):\n",
    "        model.train()\n",
    "        train_losses, nums = [], []\n",
    "#         for xb,yb in tqdm(data.train_dl, total=len(data.train_dl), unit='batches'):\n",
    "        for xb,yb in data.train_dl:\n",
    "            if scd is not None:\n",
    "                for g in opt.param_groups: g['lr'] = scd[0][nb]\n",
    "                for g in opt.param_groups: g['betas'] = (scd[1][nb], b2) # only beta1 is scaled\n",
    "                nb += 1\n",
    "            loss = loss_batch(model, xb, yb, loss_fn, opt)\n",
    "            train_losses.append(loss)\n",
    "            nums.append(xb.shape[0])\n",
    "        nums = np.array(nums, dtype=np.float32)\n",
    "        train_loss = (np.stack(train_losses) * nums).sum() / nums.sum()\n",
    "        valid_loss = validate(model, data.valid_dl, loss_fn, average=True)\n",
    "        print(f\"Epoch {e} -- train_loss: {train_loss}, valid_loss: {valid_loss}\")\n",
    "    print('done!')\n",
    "\n",
    "def annealing_cos(start, end, pct):\n",
    "    \"Cosine anneal from `start` to `end` as pct goes from 0.0 to 1.0.\"\n",
    "    cos_out = np.cos(np.pi * pct) + 1\n",
    "    return end + (start-end)/2 * cos_out\n",
    "    \n",
    "class learner():\n",
    "    def __init__(self, model, data, loss_fn, opt=optim.Adam):\n",
    "        self.m, self.data, self.loss_fn = model, data, loss_fn\n",
    "        self.opt = opt\n",
    "    \n",
    "    def OneCycleScheduler(self, epochs, lr, pct_start, moms, div=25):\n",
    "        final_div=div*1e4\n",
    "        lr_low=lr/div\n",
    "        n_batches = len(self.data.train_dl)*epochs\n",
    "        ph1 = int(n_batches * pct_start)\n",
    "        ph2 = n_batches-ph1\n",
    "        \n",
    "        def steps(start, end, ph1, ph2, final_div):\n",
    "            up = [annealing_cos(start, end, n/ph1) for n in range(ph1)]\n",
    "            down = [annealing_cos(end, end/final_div, n/ph2) for n in range(ph2)]\n",
    "            return up+down\n",
    "        \n",
    "        lrs = steps(lr_low, lr, ph1, ph2, final_div)\n",
    "        moms = steps(moms[0], moms[1], ph1, ph2, final_div)\n",
    "        return [lrs, moms]\n",
    "    \n",
    "    def fit(self, epochs, lr=1e-3, wd=0):\n",
    "        opt = self.opt(self.m.parameters(), lr=lr, weight_decay=wd)\n",
    "        fit(self.m, self.data, self.loss_fn, opt, epochs)  \n",
    "\n",
    "    def fit1cycle(self, epochs, wd=0, lr=1e-2, pct_start=0.3, moms=(0.95,0.85), div=25):\n",
    "        self.scd = self.OneCycleScheduler(epochs, lr, pct_start, moms, div)\n",
    "        opt = self.opt(self.m.parameters(), lr=lr, weight_decay=wd)\n",
    "        fit(self.m, self.data, self.loss_fn, opt, epochs, scd=self.scd)  \n",
    "        \n",
    "    def plot_scd(self):\n",
    "        fig, ax = plt.subplot(1,2)\n",
    "        ax[0,0] = plt.plot(range(len(self.scd[0])), self.scd[0])\n",
    "        ax[0,1] = plt.plot(range(len(self.scd[1])), self.scd[1])\n",
    "        \n",
    "    def predict(self, dl):\n",
    "        return validate(self.m, dl, loss_fn=None, average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MOA_data(x_fts, y_fts)\n",
    "data.create(trnval_df_rdy, val_idxs, test=test_df_rdy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ni = data.train_ds.x.shape[1]\n",
    "layers = [512, 512, 256, 128]\n",
    "out_sz = 206\n",
    "\n",
    "m = SimpleNet(ni, layers, out_sz, ps=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "learn = learner(m, data, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 -- train_loss: 0.33201444149017334, valid_loss: 0.04231078177690506\n",
      "Epoch 1 -- train_loss: 0.03843633830547333, valid_loss: 0.02162717655301094\n",
      "Epoch 2 -- train_loss: 0.019825752824544907, valid_loss: 0.01784510537981987\n",
      "Epoch 3 -- train_loss: 0.01827012561261654, valid_loss: 0.016924018040299416\n",
      "Epoch 4 -- train_loss: 0.017580796033143997, valid_loss: 0.01658945344388485\n",
      "Epoch 5 -- train_loss: 0.017116185277700424, valid_loss: 0.016308646649122238\n",
      "Epoch 6 -- train_loss: 0.016701877117156982, valid_loss: 0.01637161336839199\n",
      "Epoch 7 -- train_loss: 0.016411883756518364, valid_loss: 0.015910614281892776\n",
      "Epoch 8 -- train_loss: 0.016105279326438904, valid_loss: 0.015780121088027954\n",
      "Epoch 9 -- train_loss: 0.016014257445931435, valid_loss: 0.01576509326696396\n",
      "\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "learn.fit1cycle(10, lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "589fa432b70c468dbf81641ce57057d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 -- train_loss: 0.015949761494994164, valid_loss: 0.01575574465095997\n",
      "Epoch 1 -- train_loss: 0.015949904918670654, valid_loss: 0.015779739245772362\n",
      "Epoch 2 -- train_loss: 0.016075219959020615, valid_loss: 0.016111915931105614\n",
      "Epoch 3 -- train_loss: 0.01596575602889061, valid_loss: 0.0156718697398901\n",
      "Epoch 4 -- train_loss: 0.01568703167140484, valid_loss: 0.0158403217792511\n",
      "Epoch 5 -- train_loss: 0.015408377163112164, valid_loss: 0.01554021518677473\n",
      "Epoch 6 -- train_loss: 0.015143292024731636, valid_loss: 0.015404722653329372\n",
      "Epoch 7 -- train_loss: 0.014813438057899475, valid_loss: 0.015429357998073101\n",
      "Epoch 8 -- train_loss: 0.01453423872590065, valid_loss: 0.015403523109853268\n",
      "Epoch 9 -- train_loss: 0.014419391751289368, valid_loss: 0.01540491171181202\n",
      "\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "learn.fit1cycle(10, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b6a3b5d43d45be8c1f9fb4c082ac5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 -- train_loss: 0.20949962735176086, valid_loss: 0.02265077643096447\n",
      "Epoch 1 -- train_loss: 0.021143915131688118, valid_loss: 0.019176403060555458\n",
      "Epoch 2 -- train_loss: 0.01928352750837803, valid_loss: 0.018533499911427498\n",
      "Epoch 3 -- train_loss: 0.01886015385389328, valid_loss: 0.01786782406270504\n",
      "Epoch 4 -- train_loss: 0.018388397991657257, valid_loss: 0.017606500536203384\n",
      "Epoch 5 -- train_loss: 0.018018893897533417, valid_loss: 0.01731470413506031\n",
      "Epoch 6 -- train_loss: 0.01776273362338543, valid_loss: 0.017182406038045883\n",
      "Epoch 7 -- train_loss: 0.01758640445768833, valid_loss: 0.017024202272295952\n",
      "Epoch 8 -- train_loss: 0.017436543479561806, valid_loss: 0.016882050782442093\n",
      "Epoch 9 -- train_loss: 0.017361700534820557, valid_loss: 0.01692408137023449\n",
      "Epoch 10 -- train_loss: 0.017262758687138557, valid_loss: 0.01688043214380741\n",
      "Epoch 11 -- train_loss: 0.017195701599121094, valid_loss: 0.01683911681175232\n",
      "Epoch 12 -- train_loss: 0.01717359758913517, valid_loss: 0.016626400873064995\n",
      "Epoch 13 -- train_loss: 0.017174966633319855, valid_loss: 0.01667691208422184\n",
      "Epoch 14 -- train_loss: 0.017134331166744232, valid_loss: 0.016527920961380005\n",
      "Epoch 15 -- train_loss: 0.0170816108584404, valid_loss: 0.016555514186620712\n",
      "Epoch 16 -- train_loss: 0.017046937718987465, valid_loss: 0.016455136239528656\n",
      "Epoch 17 -- train_loss: 0.017160890623927116, valid_loss: 0.016691533848643303\n",
      "Epoch 18 -- train_loss: 0.017037609592080116, valid_loss: 0.016346698626875877\n",
      "Epoch 19 -- train_loss: 0.017107738181948662, valid_loss: 0.016462959349155426\n",
      "Epoch 20 -- train_loss: 0.0170658677816391, valid_loss: 0.016499541699886322\n",
      "Epoch 21 -- train_loss: 0.017088372260332108, valid_loss: 0.016972968354821205\n",
      "Epoch 22 -- train_loss: 0.01710047572851181, valid_loss: 0.01647472381591797\n",
      "Epoch 23 -- train_loss: 0.017138874158263206, valid_loss: 0.016584232449531555\n",
      "Epoch 24 -- train_loss: 0.0171135812997818, valid_loss: 0.01673455722630024\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "learn.fit(25, lr=1e-2, wd=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a70314b7a564da8b1c802f79a3d0b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 -- train_loss: 0.016941102221608162, valid_loss: 0.016247620806097984\n",
      "Epoch 1 -- train_loss: 0.016665706411004066, valid_loss: 0.01618214137852192\n",
      "Epoch 2 -- train_loss: 0.016668150201439857, valid_loss: 0.016295522451400757\n",
      "Epoch 3 -- train_loss: 0.016658030450344086, valid_loss: 0.016031434759497643\n",
      "Epoch 4 -- train_loss: 0.016613837331533432, valid_loss: 0.016173968091607094\n",
      "Epoch 5 -- train_loss: 0.016593245789408684, valid_loss: 0.016228972002863884\n",
      "Epoch 6 -- train_loss: 0.0166561771184206, valid_loss: 0.016130512580275536\n",
      "Epoch 7 -- train_loss: 0.016535378992557526, valid_loss: 0.015919595956802368\n",
      "Epoch 8 -- train_loss: 0.01656779833137989, valid_loss: 0.016008682548999786\n",
      "Epoch 9 -- train_loss: 0.01655224896967411, valid_loss: 0.01607232168316841\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "learn.fit(10, lr=5e-3, wd=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 -- train_loss: 0.2182941883802414, valid_loss: 0.022483980283141136\n",
      "Epoch 1 -- train_loss: 0.0220461618155241, valid_loss: 0.019376760348677635\n",
      "Epoch 2 -- train_loss: 0.020111022517085075, valid_loss: 0.01855853758752346\n",
      "Epoch 3 -- train_loss: 0.019594714045524597, valid_loss: 0.01825687848031521\n",
      "Epoch 4 -- train_loss: 0.019305594265460968, valid_loss: 0.017978398129343987\n",
      "Epoch 5 -- train_loss: 0.018953988328576088, valid_loss: 0.017496727406978607\n",
      "Epoch 6 -- train_loss: 0.018582912161946297, valid_loss: 0.017173688858747482\n",
      "Epoch 7 -- train_loss: 0.01827169395983219, valid_loss: 0.01691191829741001\n",
      "Epoch 8 -- train_loss: 0.017977101728320122, valid_loss: 0.016689136624336243\n",
      "Epoch 9 -- train_loss: 0.017696768045425415, valid_loss: 0.01653870940208435\n",
      "Epoch 10 -- train_loss: 0.017501598224043846, valid_loss: 0.016383958980441093\n",
      "Epoch 11 -- train_loss: 0.017315631732344627, valid_loss: 0.016210580244660378\n",
      "Epoch 12 -- train_loss: 0.01715618185698986, valid_loss: 0.01605110615491867\n",
      "Epoch 13 -- train_loss: 0.017005199566483498, valid_loss: 0.01603170856833458\n",
      "Epoch 14 -- train_loss: 0.016803018748760223, valid_loss: 0.01589319109916687\n",
      "Epoch 15 -- train_loss: 0.0166951734572649, valid_loss: 0.0158604234457016\n",
      "Epoch 16 -- train_loss: 0.016555562615394592, valid_loss: 0.015769852325320244\n",
      "Epoch 17 -- train_loss: 0.01637679524719715, valid_loss: 0.015703583136200905\n",
      "Epoch 18 -- train_loss: 0.01622389256954193, valid_loss: 0.015626130625605583\n",
      "Epoch 19 -- train_loss: 0.016141608357429504, valid_loss: 0.015539295971393585\n",
      "Epoch 20 -- train_loss: 0.016019757837057114, valid_loss: 0.01556814182549715\n",
      "Epoch 21 -- train_loss: 0.015877945348620415, valid_loss: 0.015501908026635647\n",
      "Epoch 22 -- train_loss: 0.015755528584122658, valid_loss: 0.015500334091484547\n",
      "Epoch 23 -- train_loss: 0.01567418873310089, valid_loss: 0.015487457625567913\n",
      "Epoch 24 -- train_loss: 0.015594035387039185, valid_loss: 0.015424234792590141\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "learn.fit(25, lr=1e-2, wd=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(res_list):\n",
    "    preds = np.vstack([p[0] for p in res_list])\n",
    "    preds = nn.Sigmoid()(torch.tensor(preds))\n",
    "    y = np.vstack([p[1] for p in res_list])\n",
    "    return [preds, y]\n",
    "\n",
    "def eval_model(learn_obj):\n",
    "    res = {'train preds': unpack(learn_obj.predict(learn_obj.data.fix_dl)), \n",
    "           'valid preds': unpack(learn_obj.predict(learn_obj.data.valid_dl)), \n",
    "           'train baseline': [learn_obj.data.train_ds.y, np.zeros(learn_obj.data.train_ds.y.shape)],\n",
    "           'valid baseline': [learn_obj.data.valid_ds.y, np.zeros(learn_obj.data.valid_ds.y.shape)]}\n",
    "        \n",
    "    return res['train preds'], res['valid preds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_res, valid_res = eval_model(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = to_np(train_res[0])\n",
    "train_y = train_res[1]\n",
    "\n",
    "valid_pred = to_np(valid_res[0])\n",
    "valid_y = valid_res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train log_loss: 2.1209915777456496\n",
      "train baseline: 3.7746580695642495\n",
      "\n",
      "\n",
      "valid log_loss: 2.465788588763078\n",
      "valid baseline: 3.712961561792907\n"
     ]
    }
   ],
   "source": [
    "print(f\"train log_loss: {metrics.log_loss(train_y, train_pred)}\")\n",
    "print(f\"train baseline: {metrics.log_loss(train_y, np.zeros(train_y.shape))}\")\n",
    "print('\\n')\n",
    "print(f\"valid log_loss: {metrics.log_loss(valid_y, valid_pred)}\")\n",
    "print(f\"valid baseline: {metrics.log_loss(valid_y, np.zeros(valid_y.shape))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission_df(preds, ref_df, smplsub):\n",
    "    cols = smplsub.columns\n",
    "    res = pd.DataFrame(preds, columns=cols[1:])\n",
    "    res['sig_id'] = list(ref_df['sig_id'])\n",
    "    pred_df = res[cols]\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_trn_x = ref_train_x.drop(val_idxs).copy()\n",
    "ref_trn_y = ref_train_y.drop(val_idxs).copy()\n",
    "\n",
    "ref_val_x = ref_train_x.loc[val_idxs].copy()\n",
    "ref_val_y = ref_train_y.loc[val_idxs].copy()\n",
    "\n",
    "train_pred_df = submission_df(train_pred, ref_trn_x, smplsub)\n",
    "valid_pred_df = submission_df(valid_pred, ref_val_x, smplsub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train zeros: 0.0\n",
      "train non-zeros: 4.568610033906662\n",
      "train zeros: 0.0\n",
      "train non-zeros: 5.31293923433169\n"
     ]
    }
   ],
   "source": [
    "z_trn_pred = train_pred_df.iloc[:,1:].to_numpy()[ref_trn_y.sum(axis=1)==0]\n",
    "z_trn_y = ref_trn_y.iloc[:,1:].to_numpy()[ref_trn_y.sum(axis=1)==0]\n",
    "nz_trn_pred = train_pred_df.iloc[:,1:].to_numpy()[ref_trn_y.sum(axis=1)>1]\n",
    "nz_trn_y = ref_trn_y.iloc[:,1:].to_numpy()[ref_trn_y.sum(axis=1)>1]\n",
    "print(f\"train zeros: {metrics.log_loss(z_trn_y,z_trn_pred)}\")\n",
    "print(f\"train non-zeros: {metrics.log_loss(nz_trn_y,nz_trn_pred)}\")\n",
    "\n",
    "z_val_pred = valid_pred_df.iloc[:,1:].to_numpy()[ref_val_y.sum(axis=1)==0]\n",
    "z_val_y = ref_val_y.iloc[:,1:].to_numpy()[ref_val_y.sum(axis=1)==0]\n",
    "nz_val_pred = valid_pred_df.iloc[:,1:].to_numpy()[ref_val_y.sum(axis=1)>1]\n",
    "nz_val_y = ref_val_y.iloc[:,1:].to_numpy()[ref_val_y.sum(axis=1)>1]\n",
    "print(f\"train zeros: {metrics.log_loss(z_val_y,z_val_pred)}\")\n",
    "print(f\"train non-zeros: {metrics.log_loss(nz_val_y,nz_val_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ctrls:  0.0\n",
      "treat: 2.301275310561092\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "trn_true_y = pd.merge(ref_trn_x[['sig_id', 'cp_type']], ref_train_y, on='sig_id')\n",
    "trn_pred_y = pd.merge(ref_trn_x[['sig_id', 'cp_type']], train_pred_df, on='sig_id')\n",
    "\n",
    "ctrl_true = trn_true_y.loc[trn_true_y['cp_type'] == 'ctl_vehicle']\n",
    "ctrl_pred = trn_pred_y.loc[trn_pred_y['cp_type'] == 'ctl_vehicle']\n",
    "print('ctrls: ',metrics.log_loss(ctrl_true.iloc[:,2:].to_numpy(), ctrl_pred.iloc[:,2:].to_numpy()))\n",
    "\n",
    "non_ctrl_true = trn_true_y.loc[trn_true_y['cp_type'] != 'ctl_vehicle']\n",
    "non_ctrl_pred = trn_pred_y.loc[trn_pred_y['cp_type'] != 'ctl_vehicle']\n",
    "print('treat:', metrics.log_loss(non_ctrl_true.iloc[:,2:].to_numpy(), non_ctrl_pred.iloc[:,2:].to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ctrls:  0.0\n",
      "treat: 2.6758580317124205\n"
     ]
    }
   ],
   "source": [
    "# val\n",
    "val_true_y = pd.merge(ref_val_x[['sig_id', 'cp_type']], ref_train_y, on='sig_id')\n",
    "val_pred_y = pd.merge(ref_val_x[['sig_id', 'cp_type']], valid_pred_df, on='sig_id')\n",
    "\n",
    "\n",
    "val_ctrl_true = val_true_y.loc[val_true_y['cp_type'] == 'ctl_vehicle']\n",
    "val_ctrl_pred = val_pred_y.loc[val_pred_y['cp_type'] == 'ctl_vehicle']\n",
    "print('ctrls: ',metrics.log_loss(val_ctrl_true.iloc[:,2:].to_numpy(), val_ctrl_pred.iloc[:,2:].to_numpy()))\n",
    "\n",
    "val_non_ctrl_true = val_true_y.loc[val_true_y['cp_type'] != 'ctl_vehicle']\n",
    "val_non_ctrl_pred = val_pred_y.loc[val_pred_y['cp_type'] != 'ctl_vehicle']\n",
    "print('treat:', metrics.log_loss(val_non_ctrl_true.iloc[:,2:].to_numpy(), val_non_ctrl_pred.iloc[:,2:].to_numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Forest: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def eval_model(forest):\n",
    "    res = {'train preds': [f_train_y.to_numpy(), forest.predict(f_train_x)], \n",
    "           'valid preds': [f_valid_y.to_numpy(), forest.predict(f_valid_x)], \n",
    "           'train baseline': [f_train_y.to_numpy(), np.zeros(f_train_y.shape)],\n",
    "           'valid baseline': [f_valid_y.to_numpy(), np.zeros(f_valid_y.shape)]}\n",
    "     \n",
    "    for key, value in res.items(): \n",
    "        print(f\"{key}: {metrics.log_loss(*value)}\")\n",
    "        \n",
    "    return res['train preds'], res['valid preds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f_valid_x = trnval_df_rdy[x_fts].loc[val_idxs]\n",
    "f_valid_y = trnval_df_rdy[y_fts].loc[val_idxs]\n",
    "\n",
    "f_train_x = trnval_df_rdy[x_fts].copy().drop(val_idxs)\n",
    "f_train_y = trnval_df_rdy[y_fts].copy().drop(val_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clf = OneVsRestClassifier(xgb.XGBClassifier(n_estimators=10, n_jobs=-1, max_depth=3, verbosity=1))\n",
    "%time clf.fit(f_train_x, f_train_y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train preds: 3.3517202616769346\n",
      "valid preds: 3.3995132701742037\n",
      "train baseline: 3.776149636240842\n",
      "valid baseline: 3.699541218798475\n"
     ]
    }
   ],
   "source": [
    "# n_estimators=10, n_jobs=-1, max_depth=3, verbosity=1\n",
    "train_preds, valid_preds = eval_model(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "non_ctrl_pred_arr = non_ctrl_pred.iloc[:,2:].to_numpy()\n",
    "non_ctrl_true_arr = non_ctrl_true.iloc[:,2:].to_numpy()\n",
    "\n",
    "val_non_ctrl_pred_arr = val_non_ctrl_pred.iloc[:,2:].to_numpy()\n",
    "val_non_ctrl_true_arr = val_non_ctrl_true.iloc[:,2:].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def opt_th(targs, preds, start=1e-7, end=1e-5, step=2e-7):\n",
    "    ths = np.arange(start,end,step)\n",
    "    res = [metrics.log_loss(targs, (preds > th)*preds) for th in ths]\n",
    "    idx = np.argmin(res)\n",
    "    return ths[idx], res[idx]\n",
    "\n",
    "def ths_binarize(arr, ths):\n",
    "    arr = arr.copy()\n",
    "    arr[arr < ths] = 0\n",
    "    arr[arr > ths] = 1\n",
    "    return arr\n",
    "\n",
    "def opt_th_binarize(targs, preds, start=5e-7, end=0.1, step=5e-7):\n",
    "    ths = np.arange(start,end,step)\n",
    "    res = [metrics.log_loss(targs, ths_binarize(preds, th)) for th in ths]\n",
    "    idx = np.argmin(res)\n",
    "    return ths[idx], res[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trn_res = opt_th_binarize(non_ctrl_true_arr, non_ctrl_pred_arr)\n",
    "print(f\"Optimal threshold train: {trn_res}\")\n",
    "\n",
    "val_res = opt_th_binarize(val_non_ctrl_true_arr, val_non_ctrl_pred_arr)\n",
    "print(f\"Optimal threshold valid: {val_res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trn_res = opt_th(non_ctrl_true_arr, non_ctrl_pred_arr)\n",
    "print(f\"Optimal threshold train: {trn_res}\")\n",
    "\n",
    "val_res = opt_th(val_non_ctrl_true_arr, val_non_ctrl_pred_arr)\n",
    "print(f\"Optimal threshold valid: {val_res}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
