{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch, torch.nn.functional as F\n",
    "from torch import ByteTensor, DoubleTensor, FloatTensor, HalfTensor, LongTensor, ShortTensor, Tensor\n",
    "from torch import nn, optim, as_tensor\n",
    "from torch.utils.data import BatchSampler, DataLoader, Dataset, Sampler, TensorDataset\n",
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "from collections.abc import Iterable\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_train_x = pd.read_csv('data/train_features.csv')\n",
    "ref_train_y = pd.read_csv('data/train_targets_scored.csv')\n",
    "ref_train_y2 = pd.read_csv('data/train_targets_nonscored.csv')\n",
    "\n",
    "ref_test_x = pd.read_csv('data/test_features.csv')\n",
    "smplsub = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all IDs unique?\n",
    "((ref_train_x['sig_id'].value_counts()) == 1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ref_train_y2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-d7e4d0a5d429>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_y_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mref_train_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref_train_y2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sig_id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain_y_all\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ctrl'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mref_train_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cp_type'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'ctl_vehicle'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrain_y_all\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'other'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_y_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mref_train_y2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtrain_y_all\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'zero'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_y_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ref_train_y2' is not defined"
     ]
    }
   ],
   "source": [
    "train_y_all = pd.merge(ref_train_y, ref_train_y2, on='sig_id')\n",
    "train_y_all['ctrl'] = (ref_train_x['cp_type'] == 'ctl_vehicle').astype(int)\n",
    "train_y_all['other'] = (train_y_all.loc[:,ref_train_y2.columns].sum(axis=1) > 0).astype(int)\n",
    "train_y_all['zero'] = (train_y_all.sum(axis=1) == 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_train_y = train_y_all.loc[:,list(ref_train_y.columns)+['ctrl', 'zero', 'other']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_col(df, col):\n",
    "    enc = pd.get_dummies(df[col])\n",
    "    enc.columns = [f\"{col}_{n}\" for n in enc.columns]\n",
    "    df = df.drop(col, axis=1)\n",
    "    df = df.join(enc)\n",
    "    return df\n",
    "\n",
    "def prep_data(df, cols, func=onehot_col):\n",
    "    for i in cols: df = func(df, i)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # non_ctrl only:\n",
    "# _ref_train_x_trt = ref_train_x[ref_train_x['cp_type'] != 'ctl_vehicle'].drop('cp_type', axis=1)\n",
    "# _ref_test_x_trt = ref_test_x[ref_test_x['cp_type'] != 'ctl_vehicle'].drop('cp_type', axis=1)\n",
    "\n",
    "# non_zero only:\n",
    "_ref_train_x_trt = ref_train_x[ref_train_x.sum(axis=1) > 0].drop('cp_type', axis=1)\n",
    "_ref_test_x_trt = ref_test_x[ref_test_x.sum(axis=1) > 0].drop('cp_type', axis=1)\n",
    "\n",
    "# normal prep\n",
    "trt_ref_train_x = prep_data(_ref_train_x_trt, cols=['cp_time', 'cp_dose'])\n",
    "trt_ref_test_x = prep_data(_ref_test_x_trt, cols=['cp_time', 'cp_dose'])\n",
    "\n",
    "x_fts = trt_ref_train_x.columns[1:]\n",
    "y_fts = ref_train_y.columns[1:]\n",
    "\n",
    "trt_trnval_df_rdy = pd.merge(trt_ref_train_x, ref_train_y, on='sig_id')\n",
    "trt_test_df_rdy = trt_ref_test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(_ref_train_x_trt.shape, ref_train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_test_df_rdy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_stratified_val_idxs(df, val_size=0.1, rnd=0):\n",
    "    \n",
    "    arr = df.to_numpy()\n",
    "\n",
    "    X = arr[:,0]\n",
    "    y = arr[:,1:] # this works irrespective of whether labels are space- or comma-separated\n",
    "    \n",
    "    ### sklearn.model_selection.StratifiedKFold\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=rnd)\n",
    "    \n",
    "    for train_index, val_index in sss.split(X, y):\n",
    "        trn_idxs = train_index\n",
    "        val_idxs = val_index\n",
    "\n",
    "    data_report(df, trn_idxs, val_idxs)\n",
    "    return trn_idxs, val_idxs\n",
    "\n",
    "def finalize_df(df, targets, as_multi=True): \n",
    "    # Select and fuse labels into target column (space separated)\n",
    "    df_slct = df[[df.columns[0]] + targets]\n",
    "    if as_multi:\n",
    "        df_out = np.array([[df_slct.values[i][0], ' '.join(str(x) for x in df_slct.values[i][1:])] for i in range(len(df_slct))])\n",
    "        return pd.DataFrame(df_out, columns = [\"ID\", \"Target\"])\n",
    "    else: \n",
    "        df_out = np.array(df_slct)\n",
    "        if len(targets) == 1: return pd.DataFrame(df_out, columns = [\"ID\", 'Target'])\n",
    "        else: return pd.DataFrame(df_out, columns = [\"ID\"] + targets)\n",
    "\n",
    "def data_report(df, trn_idxs, val_idxs, test_csv=None):\n",
    "    trnval = df\n",
    "    if len(trnval.columns) != 2:\n",
    "        print(f\"Multilabel csv with comma-separated labels detected!\\n\")\n",
    "        trnval = finalize_df(trnval, targets=list(trnval.columns)[1:])\n",
    "    print(f\"\"\"Train label-distribution:\\n\"\"\"\n",
    "          f\"\"\"{trnval['Target'][trn_idxs].value_counts()}\\n\"\"\"\n",
    "          f\"\"\"Total: {len(trn_idxs)}\\n\"\"\")\n",
    "    print(f\"\"\"Val label-distribution:\\n\"\"\"\n",
    "          f\"\"\"{trnval['Target'][val_idxs].value_counts()}\\n\"\"\"\n",
    "          f\"\"\"Total: {len(val_idxs)}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_idxs, val_idxs = get_label_stratified_val_idxs(_ref_train_x_trt.iloc[:,:3], val_size=0.1, rnd=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - normlize fts\n",
    "# - embed cat fts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MOA_data:\n",
    "    def __init__(self, x_fts, y_fts, bs=512):\n",
    "        self.x_fts, self.y_fts, self.bs = x_fts, y_fts, bs\n",
    "    \n",
    "    def embed(df):\n",
    "        return df\n",
    "\n",
    "    def create(self, df, val_idxs, test=None): \n",
    "        train = df.drop(val_idxs)\n",
    "        valid = df.loc[val_idxs]\n",
    "        for ID in valid['sig_id']: assert ID not in list(train['sig_id']) \n",
    "        self.train_ds = MOA_ds(train, self.x_fts, self.y_fts)\n",
    "        self.valid_ds = MOA_ds(valid, self.x_fts, self.y_fts)\n",
    "        \n",
    "        self.train_dl = DataLoader(self.train_ds, batch_size=self.bs, shuffle=True)\n",
    "        self.valid_dl = DataLoader(self.valid_ds, batch_size=self.bs, shuffle=False)\n",
    "        self.fix_dl = DataLoader(self.train_ds, batch_size=self.bs, shuffle=False)\n",
    "        \n",
    "        if test is not None:\n",
    "            self.test_ds = MOA_ds(test, self.x_fts, y_fts, test=True)\n",
    "            self.test_dl = DataLoader(self.test_ds, batch_size=self.bs, shuffle=False)\n",
    "\n",
    "class MOA_ds(Dataset):\n",
    "    def __init__(self, df, x_fts, y_fts, test=False):\n",
    "        if test: self.x, self.y = df[x_fts].to_numpy(), np.zeros((df.shape[0], len(y_fts)))\n",
    "        else: self.x, self.y = df[x_fts].to_numpy(), df[y_fts].to_numpy()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return [torch.tensor(self.x[idx, :], dtype=torch.float),\n",
    "                torch.tensor(self.y[idx, :], dtype=torch.float)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MOA_data(x_fts, y_fts)\n",
    "# data.create(trnval_df_rdy, val_idxs, test=test_df_rdy)\n",
    "data.create(trt_trnval_df_rdy, val_idxs, test=trt_test_df_rdy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ifnone(a,b):\n",
    "    \"`a` if `a` is not None, otherwise `b`.\"\n",
    "    return b if a is None else a\n",
    "\n",
    "def listify(p=None, q=None):\n",
    "    \"Make `p` listy and the same length as `q`.\"\n",
    "    if p is None: p=[]\n",
    "    elif isinstance(p, str):          p = [p]\n",
    "    elif not isinstance(p, Iterable): p = [p]\n",
    "    #Rank 0 tensors in PyTorch are Iterable but don't have a length.\n",
    "    else:\n",
    "        try: a = len(p)\n",
    "        except: p = [p]\n",
    "    n = q if type(q)==int else len(p) if q is None else len(q)\n",
    "    if len(p)==1: p = p * n\n",
    "    assert len(p)==n, f'List len mismatch ({len(p)} vs {n})'\n",
    "    return list(p)\n",
    "\n",
    "def emb_sz_rule(n_cat:int)->int: return min(600, round(1.6 * n_cat**0.56))\n",
    "\n",
    "def def_emb_sz(classes, n, sz_dict=None):\n",
    "    \"Pick an embedding size for `n` depending on `classes` if not given in `sz_dict`.\"\n",
    "    sz_dict = ifnone(sz_dict, {})\n",
    "    n_cat = len(classes[n])\n",
    "    sz = sz_dict.get(n, int(emb_sz_rule(n_cat)))  # rule of thumb\n",
    "    return n_cat,sz\n",
    "\n",
    "def get_emb_szs(self, sz_dict=None):\n",
    "    \"Return the default embedding sizes suitable for this data or takes the ones in `sz_dict`.\"\n",
    "    return [def_emb_sz(self.classes, n, sz_dict) for n in self.cat_names]\n",
    "\n",
    "def embedding(ni,nf):\n",
    "    \"Create an embedding layer.\"\n",
    "    emb = nn.Embedding(ni, nf)\n",
    "    with torch.no_grad(): trunc_normal_(emb.weight, std=0.01)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_drop_lin(n_in, n_out, bn=True, p=0., actn=None):\n",
    "    \"Sequence of batchnorm (if `bn`), dropout (with `p`) and linear (`n_in`,`n_out`) layers followed by `actn`.\"\n",
    "    layers = [nn.BatchNorm1d(n_in)] if bn else []\n",
    "    if p != 0: layers.append(nn.Dropout(p))\n",
    "    layers.append(nn.Linear(n_in, n_out))\n",
    "    if actn is not None: layers.append(actn)\n",
    "    return layers\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, in_fts, layers, out_sz, ps=None, use_bn=True, bn_final=False):\n",
    "        super().__init__()\n",
    "        ps = ifnone(ps, [0]*len(layers))\n",
    "        ps = listify(ps, layers)\n",
    "        sizes = [in_fts] + layers + [out_sz]\n",
    "        actns = [nn.ReLU(inplace=True) for _ in range(len(sizes)-2)] + [None]\n",
    "        layers = []\n",
    "        \n",
    "        for i,(n_in,n_out,dp,act) in enumerate(zip(sizes[:-1],sizes[1:],[0.]+ps,actns)):\n",
    "            layers += bn_drop_lin(n_in, n_out, bn=use_bn and i!=0, p=dp, actn=act)\n",
    "        if bn_final: layers.append(nn.BatchNorm1d(sizes[-1]))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNet(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=877, out_features=512, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.3)\n",
       "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.3)\n",
       "    (8): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (9): ReLU(inplace)\n",
       "    (10): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Dropout(p=0.3)\n",
       "    (12): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (13): ReLU(inplace)\n",
       "    (14): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (15): Dropout(p=0.3)\n",
       "    (16): Linear(in_features=128, out_features=206, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ni = data.train_ds.x.shape[1]\n",
    "layers = [512, 512, 256, 128]\n",
    "out_sz = 206\n",
    "\n",
    "m = SimpleNet(ni, layers, out_sz, ps=0.3)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_np(x): \n",
    "    return x.data.cpu().numpy()\n",
    "\n",
    "def loss_batch(model, x, y, loss_func, opt=None): \n",
    "    out = model(x)\n",
    "    if not loss_func: return to_np(out), to_np(y)\n",
    "    loss = loss_func(out, y)\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    return loss.detach().cpu()    \n",
    "    \n",
    "def validate(model, dl, loss_fn=None, average=True):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses,nums = [],[]\n",
    "#         for xb,yb in tqdm(dl, total=len(dl), unit='batches'):\n",
    "        for xb,yb in dl:\n",
    "            val_loss = loss_batch(model, xb, yb, loss_fn)\n",
    "            val_losses.append(val_loss)\n",
    "            nums.append(xb.shape[0])\n",
    "        nums = np.array(nums, dtype=np.float32)\n",
    "        if average: return (to_np(torch.stack(val_losses)) * nums).sum() / nums.sum()\n",
    "        else:       return val_losses\n",
    "        \n",
    "def fit(model, data, loss_fn, opt, epochs, average=True, scd=None):\n",
    "    nb = 0\n",
    "    b2 = opt.param_groups[0]['betas'][1]\n",
    "    for e in tqdm(range(epochs), total=epochs, unit='epochs'):\n",
    "        model.train()\n",
    "        train_losses, nums = [], []\n",
    "#         for xb,yb in tqdm(data.train_dl, total=len(data.train_dl), unit='batches'):\n",
    "        for xb,yb in data.train_dl:\n",
    "            if scd is not None:\n",
    "                for g in opt.param_groups: g['lr'] = scd[0][nb]\n",
    "                for g in opt.param_groups: g['betas'] = (scd[1][nb], b2) # only beta1 is scaled\n",
    "                nb += 1\n",
    "            loss = loss_batch(model, xb, yb, loss_fn, opt)\n",
    "            train_losses.append(loss)\n",
    "            nums.append(xb.shape[0])\n",
    "        nums = np.array(nums, dtype=np.float32)\n",
    "        train_loss = (np.stack(train_losses) * nums).sum() / nums.sum()\n",
    "        valid_loss = validate(model, data.valid_dl, loss_fn, average=True)\n",
    "        print(f\"Epoch {e} -- train_loss: {train_loss}, valid_loss: {valid_loss}\")\n",
    "    print('done!')\n",
    "\n",
    "def annealing_cos(start, end, pct):\n",
    "    \"Cosine anneal from `start` to `end` as pct goes from 0.0 to 1.0.\"\n",
    "    cos_out = np.cos(np.pi * pct) + 1\n",
    "    return end + (start-end)/2 * cos_out\n",
    "    \n",
    "class learner():\n",
    "    def __init__(self, model, data, loss_fn, opt=optim.Adam):\n",
    "        self.m, self.data, self.loss_fn = model, data, loss_fn\n",
    "        self.opt = opt\n",
    "    \n",
    "    def OneCycleScheduler(self, epochs, lr, pct_start, moms, div=25):\n",
    "        final_div=div*1e4\n",
    "        lr_low=lr/div\n",
    "        n_batches = len(self.data.train_dl)*epochs\n",
    "        ph1 = int(n_batches * pct_start)\n",
    "        ph2 = n_batches-ph1\n",
    "        \n",
    "        def steps(start, end, ph1, ph2, final_div):\n",
    "            up = [annealing_cos(start, end, n/ph1) for n in range(ph1)]\n",
    "            down = [annealing_cos(end, end/final_div, n/ph2) for n in range(ph2)]\n",
    "            return up+down\n",
    "        \n",
    "        lrs = steps(lr_low, lr, ph1, ph2, final_div)\n",
    "        moms = steps(moms[0], moms[1], ph1, ph2, final_div)\n",
    "        return [lrs, moms]\n",
    "    \n",
    "    def fit(self, epochs, lr=1e-3, wd=0):\n",
    "        opt = self.opt(self.m.parameters(), lr=lr, weight_decay=wd)\n",
    "        fit(self.m, self.data, self.loss_fn, opt, epochs)  \n",
    "\n",
    "    def fit1cycle(self, epochs, wd=0, lr=1e-2, pct_start=0.3, moms=(0.95,0.85), div=25):\n",
    "        self.scd = self.OneCycleScheduler(epochs, lr, pct_start, moms, div)\n",
    "        opt = self.opt(self.m.parameters(), lr=lr, weight_decay=wd)\n",
    "        fit(self.m, self.data, self.loss_fn, opt, epochs, scd=self.scd)  \n",
    "        \n",
    "    def plot_scd(self):\n",
    "        fig, ax = plt.subplot(1,2)\n",
    "        ax[0,0] = plt.plot(range(len(self.scd[0])), self.scd[0])\n",
    "        ax[0,1] = plt.plot(range(len(self.scd[1])), self.scd[1])\n",
    "        \n",
    "    def predict(self, dl):\n",
    "        return validate(self.m, dl, loss_fn=None, average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MOA_data(x_fts, y_fts, bs=512)\n",
    "# data.create(trnval_df_rdy, val_idxs, test=test_df_rdy)\n",
    "data.create(trt_trnval_df_rdy, val_idxs, test=trt_test_df_rdy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "ni = data.train_ds.x.shape[1]\n",
    "layers = [512, 512, 256, 128]\n",
    "out_sz = 206\n",
    "\n",
    "m = SimpleNet(ni, layers, out_sz, ps=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = None\n",
    "gc.collect()\n",
    "\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "learn = learner(m, data, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc7968d5cc14d9c83b777ac9397471b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 -- train_loss: 0.3751540184020996, valid_loss: 0.03696988523006439\n",
      "Epoch 1 -- train_loss: 0.04447437822818756, valid_loss: 0.03561025112867355\n",
      "Epoch 2 -- train_loss: 0.023405199870467186, valid_loss: 0.018650054931640625\n",
      "Epoch 3 -- train_loss: 0.018609484657645226, valid_loss: 0.0177832692861557\n",
      "Epoch 4 -- train_loss: 0.01785828359425068, valid_loss: 0.01729653589427471\n",
      "Epoch 5 -- train_loss: 0.017542783170938492, valid_loss: 0.016987888142466545\n",
      "Epoch 6 -- train_loss: 0.017284123227000237, valid_loss: 0.017164140939712524\n",
      "Epoch 7 -- train_loss: 0.017046833410859108, valid_loss: 0.016862887889146805\n",
      "Epoch 8 -- train_loss: 0.016774510964751244, valid_loss: 0.01677710935473442\n",
      "Epoch 9 -- train_loss: 0.016629096120595932, valid_loss: 0.016783254221081734\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "learn.fit1cycle(10, lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Record: 0.01679310016334057\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(res_list):\n",
    "    preds = np.vstack([p[0] for p in res_list])\n",
    "    preds = nn.Sigmoid()(torch.tensor(preds))\n",
    "    y = np.vstack([p[1] for p in res_list])\n",
    "    return [preds, y]\n",
    "\n",
    "def eval_model(learn_obj):\n",
    "    res = {'train preds': unpack(learn_obj.predict(learn_obj.data.fix_dl)), \n",
    "           'valid preds': unpack(learn_obj.predict(learn_obj.data.valid_dl)), \n",
    "           'train baseline': [learn_obj.data.train_ds.y, np.zeros(learn_obj.data.train_ds.y.shape)],\n",
    "           'valid baseline': [learn_obj.data.valid_ds.y, np.zeros(learn_obj.data.valid_ds.y.shape)]}\n",
    "        \n",
    "    return res['train preds'], res['valid preds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_res, valid_res = eval_model(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = to_np(train_res[0])\n",
    "train_y = train_res[1]\n",
    "\n",
    "valid_pred = to_np(valid_res[0])\n",
    "valid_y = valid_res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train log_loss: 2.532457615686388\n",
      "train baseline: 3.4999552717729197\n",
      "\n",
      "\n",
      "valid log_loss: 2.634851035657406\n",
      "valid baseline: 3.412364176996556\n"
     ]
    }
   ],
   "source": [
    "print(f\"train log_loss: {metrics.log_loss(train_y, train_pred)}\")\n",
    "print(f\"train baseline: {metrics.log_loss(train_y, np.zeros(train_y.shape))}\")\n",
    "print('\\n')\n",
    "print(f\"valid log_loss: {metrics.log_loss(valid_y, valid_pred)}\")\n",
    "print(f\"valid baseline: {metrics.log_loss(valid_y, np.zeros(valid_y.shape))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Val record: 2.634851035657406\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission_df(preds, ref_df, smplsub):\n",
    "    cols = smplsub.columns\n",
    "    res = pd.DataFrame(preds, columns=cols[1:])\n",
    "    res['sig_id'] = list(ref_df['sig_id'])\n",
    "    pred_df = res[cols]\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_trn_x = ref_train_x.drop(val_idxs).copy()\n",
    "ref_val_x = ref_train_x.loc[val_idxs].copy()\n",
    "\n",
    "train_pred_df = submission_df(train_pred, ref_trn_x, smplsub)\n",
    "valid_pred_df = submission_df(valid_pred, ref_val_x, smplsub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "trn_true_y = pd.merge(ref_trn_x[['sig_id', 'cp_type']], ref_train_y, on='sig_id')\n",
    "trn_pred_y = pd.merge(ref_trn_x[['sig_id', 'cp_type']], train_pred_df, on='sig_id')\n",
    "\n",
    "ctrl_true = trn_true_y.loc[trn_true_y['cp_type'] == 'ctl_vehicle']\n",
    "ctrl_pred = trn_pred_y.loc[trn_pred_y['cp_type'] == 'ctl_vehicle']\n",
    "print('ctrls: ',metrics.log_loss(ctrl_true.iloc[:,2:].to_numpy(), ctrl_pred.iloc[:,2:].to_numpy()))\n",
    "\n",
    "non_ctrl_true = trn_true_y.loc[trn_true_y['cp_type'] != 'ctl_vehicle']\n",
    "non_ctrl_pred = trn_pred_y.loc[trn_pred_y['cp_type'] != 'ctl_vehicle']\n",
    "print('treat:', metrics.log_loss(non_ctrl_true.iloc[:,2:].to_numpy(), non_ctrl_pred.iloc[:,2:].to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val\n",
    "val_true_y = pd.merge(ref_val_x[['sig_id', 'cp_type']], ref_train_y, on='sig_id')\n",
    "val_pred_y = pd.merge(ref_val_x[['sig_id', 'cp_type']], valid_pred_df, on='sig_id')\n",
    "\n",
    "\n",
    "val_ctrl_true = val_true_y.loc[val_true_y['cp_type'] == 'ctl_vehicle']\n",
    "val_ctrl_pred = val_pred_y.loc[val_pred_y['cp_type'] == 'ctl_vehicle']\n",
    "print('ctrls: ',metrics.log_loss(val_ctrl_true.iloc[:,2:].to_numpy(), val_ctrl_pred.iloc[:,2:].to_numpy()))\n",
    "\n",
    "val_non_ctrl_true = val_true_y.loc[val_true_y['cp_type'] != 'ctl_vehicle']\n",
    "val_non_ctrl_pred = val_pred_y.loc[val_pred_y['cp_type'] != 'ctl_vehicle']\n",
    "print('treat:', metrics.log_loss(val_non_ctrl_true.iloc[:,2:].to_numpy(), val_non_ctrl_pred.iloc[:,2:].to_numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "non_ctrl_pred_arr = non_ctrl_pred.iloc[:,2:].to_numpy()\n",
    "non_ctrl_true_arr = non_ctrl_true.iloc[:,2:].to_numpy()\n",
    "\n",
    "val_non_ctrl_pred_arr = val_non_ctrl_pred.iloc[:,2:].to_numpy()\n",
    "val_non_ctrl_true_arr = val_non_ctrl_true.iloc[:,2:].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def opt_th(targs, preds, start=1e-7, end=1e-5, step=2e-7):\n",
    "    ths = np.arange(start,end,step)\n",
    "    res = [metrics.log_loss(targs, (preds > th)*preds) for th in ths]\n",
    "    idx = np.argmin(res)\n",
    "    return ths[idx], res[idx]\n",
    "\n",
    "def ths_binarize(arr, ths):\n",
    "    arr = arr.copy()\n",
    "    arr[arr < ths] = 0\n",
    "    arr[arr > ths] = 1\n",
    "    return arr\n",
    "\n",
    "def opt_th_binarize(targs, preds, start=5e-7, end=0.1, step=5e-7):\n",
    "    ths = np.arange(start,end,step)\n",
    "    res = [metrics.log_loss(targs, ths_binarize(preds, th)) for th in ths]\n",
    "    idx = np.argmin(res)\n",
    "    return ths[idx], res[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trn_res = opt_th_binarize(non_ctrl_true_arr, non_ctrl_pred_arr)\n",
    "print(f\"Optimal threshold train: {trn_res}\")\n",
    "\n",
    "val_res = opt_th_binarize(val_non_ctrl_true_arr, val_non_ctrl_pred_arr)\n",
    "print(f\"Optimal threshold valid: {val_res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trn_res = opt_th(non_ctrl_true_arr, non_ctrl_pred_arr)\n",
    "print(f\"Optimal threshold train: {trn_res}\")\n",
    "\n",
    "val_res = opt_th(val_non_ctrl_true_arr, val_non_ctrl_pred_arr)\n",
    "print(f\"Optimal threshold valid: {val_res}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
